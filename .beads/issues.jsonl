{"id":"yam-c81","title":"Yammer: Linux Dictation App","description":"# Linux Dictation Application\n\nA dictation app for Linux (Ubuntu/X11) with:\n- Speech-to-text via whisper-rs (whisper.cpp)\n- LLM-based text correction via llama_cpp-rs\n- Floating UI with waveform visualization (Tauri)\n- Chromeless, rounded-edge window design\n\n## Architecture\nPure Rust stack:\n- **STT**: whisper-rs (whisper.cpp bindings)\n- **LLM**: llama_cpp-rs (llama.cpp bindings)\n- **Audio**: cpal + rubato for resampling\n- **UI**: Tauri with transparent windows\n- **Output**: xdotool for text injection\n\n## Key Design Decisions\n1. X11 only (simplifies hotkeys, window management, text injection)\n2. CUDA support for GPU inference\n3. Model weights downloaded at runtime, not bundled in repo\n4. Phased implementation proving each building block independently\n\n## Phases\n1. Project scaffolding and model management\n2. Core audio pipeline (capture, resample, VAD)\n3. Speech-to-text integration\n4. LLM text correction\n5. Tauri UI with waveform\n6. Integration and polish","status":"open","priority":0,"issue_type":"epic","created_at":"2025-12-03T11:57:26.993833325-05:00","updated_at":"2025-12-03T11:57:26.993833325-05:00"}
{"id":"yam-c81.1","title":"Phase 1: Project Scaffolding \u0026 Model Management","description":"Set up the Rust project structure and model download infrastructure.\n\n## Goals\n- Initialize Rust workspace with proper structure\n- Create model management system that downloads weights on first run\n- Establish shared configuration for model paths\n\n## Why First\nModels are large (500MB-4GB) and must not be in the repo.\nAll subsequent phases depend on having models available.","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-03T11:57:56.062720595-05:00","updated_at":"2025-12-03T12:27:17.40563153-05:00","closed_at":"2025-12-03T12:27:17.40563153-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.1","depends_on_id":"yam-c81","type":"parent-child","created_at":"2025-12-03T11:57:56.063777659-05:00","created_by":"andy"}]}
{"id":"yam-c81.1.1","title":"Initialize Rust workspace structure","description":"## Context\nNeed a clean Rust workspace structure for the multi-crate project.\n\n## Implementation Details\nCreate workspace with:\n- `yammer-core/` - shared types, config, model management\n- `yammer-audio/` - cpal capture, resampling, VAD\n- `yammer-stt/` - whisper-rs integration\n- `yammer-llm/` - llama_cpp-rs integration\n- `yammer-cli/` - headless CLI for testing building blocks\n- `yammer-app/` - Tauri application (later phase)\n\n## Acceptance Criteria\n- [ ] Cargo.toml workspace with all crates\n- [ ] Each crate has minimal structure\n- [ ] `cargo build` succeeds\n- [ ] .gitignore excludes model files\n\n## Notes\nKeep dependencies minimal initially - add as needed per phase.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-03T11:58:14.138355793-05:00","updated_at":"2025-12-03T12:21:12.978877968-05:00","closed_at":"2025-12-03T12:21:12.978877968-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.1.1","depends_on_id":"yam-c81.1","type":"parent-child","created_at":"2025-12-03T11:58:14.140278295-05:00","created_by":"andy"}]}
{"id":"yam-c81.1.2","title":"Implement model download manager","description":"## Context\nModels (Whisper ~150-500MB, LLM ~2-4GB) must be downloaded on first run.\nCannot bundle in repo due to size.\n\n## Implementation Details\nCreate `yammer-core` module that:\n1. Defines model registry (URLs, checksums, sizes)\n2. Checks ~/.local/share/yammer/models/ for existing models\n3. Downloads missing models with progress reporting\n4. Verifies SHA256 checksums after download\n\nModels to support:\n- whisper-base.en (ggml format, ~142MB)\n- whisper-small.en (ggml format, ~466MB)\n- phi-3-mini or qwen2-1.5b (GGUF format, ~2GB)\n\n## Acceptance Criteria\n- [ ] Model registry with URLs and checksums\n- [ ] Download with progress callback\n- [ ] Checksum verification\n- [ ] CLI command: `yammer-cli download-models`\n- [ ] Skip already-downloaded models\n\n## Testing\nRun `yammer-cli download-models --dry-run` shows what would download.\nRun `yammer-cli download-models` actually downloads.\n\n## Notes\nUse reqwest for HTTP, sha2 for checksums.\nConsider HuggingFace Hub URLs for models.","status":"closed","priority":0,"issue_type":"feature","created_at":"2025-12-03T11:58:42.502935256-05:00","updated_at":"2025-12-03T12:26:39.72783074-05:00","closed_at":"2025-12-03T12:26:39.72783074-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.1.2","depends_on_id":"yam-c81.1","type":"parent-child","created_at":"2025-12-03T11:58:42.504072642-05:00","created_by":"andy"},{"issue_id":"yam-c81.1.2","depends_on_id":"yam-c81.1.1","type":"blocks","created_at":"2025-12-03T11:58:42.505224825-05:00","created_by":"andy"}]}
{"id":"yam-c81.2","title":"Phase 2: Audio Pipeline","description":"Prove out audio capture, resampling, and voice activity detection.\n\n## Goals\n- Capture microphone audio via cpal\n- Resample to 16kHz mono for Whisper\n- Implement basic VAD to detect speech vs silence\n- Save/playback test recordings\n\n## Why Phase 2\nAudio pipeline must work reliably before STT integration.\nVAD prevents processing silence and knows when utterances end.","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-03T11:58:55.589211921-05:00","updated_at":"2025-12-03T12:38:53.283847596-05:00","closed_at":"2025-12-03T12:38:53.283847596-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.2","depends_on_id":"yam-c81","type":"parent-child","created_at":"2025-12-03T11:58:55.590274297-05:00","created_by":"andy"},{"issue_id":"yam-c81.2","depends_on_id":"yam-c81.1","type":"blocks","created_at":"2025-12-03T11:58:55.591462334-05:00","created_by":"andy"}]}
{"id":"yam-c81.2.1","title":"Implement microphone capture with cpal","description":"## Context\nFirst step in audio pipeline - capture raw audio from default microphone.\n\n## Implementation Details\nIn `yammer-audio`:\n1. Initialize cpal with default host (ALSA on Linux)\n2. Get default input device\n3. Configure stream with callback\n4. Collect samples into ring buffer or channel\n\nKey considerations:\n- Handle different sample formats (f32, i16)\n- Handle different channel counts (mono, stereo → convert to mono)\n- Request smaller buffer sizes for lower latency (~10ms at 48kHz = 512 samples)\n\n## Acceptance Criteria\n- [ ] List available input devices\n- [ ] Capture audio from default device\n- [ ] Handle stream errors gracefully\n- [ ] CLI command: `yammer-cli record --duration 5s --output test.wav`\n- [ ] Captured audio plays back correctly\n\n## Testing\nRecord 5 seconds, play it back with aplay/paplay.\n\n## Notes\nBuild dependency: `sudo apt install libasound2-dev`","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-03T11:59:14.611366524-05:00","updated_at":"2025-12-03T12:32:05.600782884-05:00","closed_at":"2025-12-03T12:32:05.600782884-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.2.1","depends_on_id":"yam-c81.2","type":"parent-child","created_at":"2025-12-03T11:59:14.612677836-05:00","created_by":"andy"},{"issue_id":"yam-c81.2.1","depends_on_id":"yam-c81.1","type":"blocks","created_at":"2025-12-03T12:09:52.543495277-05:00","created_by":"andy"}]}
{"id":"yam-c81.2.2","title":"Implement audio resampling to 16kHz","description":"## Context\nWhisper requires 16kHz mono audio. Most mics capture at 44.1/48kHz.\n\n## Implementation Details\nUse rubato crate for high-quality resampling:\n1. Detect input sample rate from cpal config\n2. Create SincFixedIn resampler for real-time use\n3. Process audio chunks through resampler\n4. Output 16kHz mono f32 samples\n\nConfiguration for quality/latency tradeoff:\n- sinc_len: 256 (good quality)\n- oversampling_factor: 256\n- Linear interpolation (faster than cubic)\n\n## Acceptance Criteria\n- [ ] Resample 48kHz → 16kHz with rubato\n- [ ] Handle stereo → mono conversion\n- [ ] Process in real-time without buffer underruns\n- [ ] CLI command: `yammer-cli record --resample --output test_16k.wav`\n\n## Testing\nRecord at native rate and resampled, compare in Audacity.\nResampled audio should be 16kHz mono with no audible artifacts.\n\n## Notes\nrubato works with f64 internally, convert f32↔f64 at boundaries.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-03T11:59:37.691045249-05:00","updated_at":"2025-12-03T12:35:08.638107403-05:00","closed_at":"2025-12-03T12:35:08.638107403-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.2.2","depends_on_id":"yam-c81.2","type":"parent-child","created_at":"2025-12-03T11:59:37.692159576-05:00","created_by":"andy"},{"issue_id":"yam-c81.2.2","depends_on_id":"yam-c81.2.1","type":"blocks","created_at":"2025-12-03T11:59:37.693130287-05:00","created_by":"andy"}]}
{"id":"yam-c81.2.3","title":"Implement Voice Activity Detection (VAD)","description":"## Context\nVAD detects speech vs silence, essential for:\n- Not processing dead air (wastes GPU)\n- Knowing when an utterance ends\n- Segmenting continuous audio into chunks\n\n## Implementation Details\nStart with simple energy-based VAD:\n1. Calculate RMS (root mean square) of audio frames\n2. Compare against threshold\n3. Track speech state with hysteresis (debounce)\n\n```rust\nfn is_speech(samples: \u0026[f32], threshold: f32) -\u003e bool {\n    let rms = (samples.iter().map(|s| s * s).sum::\u003cf32\u003e() / samples.len() as f32).sqrt();\n    rms \u003e threshold\n}\n```\n\nHysteresis parameters:\n- speech_start_frames: 3 (consecutive speech to trigger start)\n- speech_end_frames: 15 (consecutive silence to trigger end)\n- min_speech_duration_ms: 250 (ignore very short sounds)\n\n## Acceptance Criteria\n- [ ] Energy-based VAD with configurable threshold\n- [ ] Hysteresis to avoid flapping\n- [ ] CLI command: `yammer-cli vad-test` (shows speech/silence in real-time)\n- [ ] Segment audio into utterances\n\n## Testing\nRun vad-test, speak, see it detect speech. Silence shows \"quiet\".\nAdjust threshold via CLI flag until it works for your mic/environment.\n\n## Notes\nLater enhancement: Silero VAD (ONNX model) is more robust to noise,\nbut energy-based is simpler and sufficient for initial testing.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-03T12:00:56.548675321-05:00","updated_at":"2025-12-03T12:38:25.812015751-05:00","closed_at":"2025-12-03T12:38:25.812015751-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.2.3","depends_on_id":"yam-c81.2","type":"parent-child","created_at":"2025-12-03T12:00:56.549667111-05:00","created_by":"andy"},{"issue_id":"yam-c81.2.3","depends_on_id":"yam-c81.2.2","type":"blocks","created_at":"2025-12-03T12:00:56.550702806-05:00","created_by":"andy"}]}
{"id":"yam-c81.3","title":"Phase 3: Speech-to-Text","description":"Prove out whisper-rs integration for speech-to-text.\n\n## Goals\n- Load Whisper model with whisper-rs\n- Transcribe audio files (batch mode)\n- Transcribe live audio chunks (streaming mode)\n- Validate CUDA acceleration works\n\n## Why Phase 3\nDepends on Phase 1 (models downloaded) and Phase 2 (audio pipeline ready).\nSTT is the core value proposition - must work reliably.","status":"closed","priority":0,"issue_type":"epic","created_at":"2025-12-03T12:01:42.494170745-05:00","updated_at":"2025-12-03T12:47:25.779638503-05:00","closed_at":"2025-12-03T12:47:25.779638503-05:00","close_reason":"Completed all Phase 3 goals: Whisper model loading, batch transcription, and streaming live audio transcription.","dependencies":[{"issue_id":"yam-c81.3","depends_on_id":"yam-c81","type":"parent-child","created_at":"2025-12-03T12:01:42.4949374-05:00","created_by":"andy"},{"issue_id":"yam-c81.3","depends_on_id":"yam-c81.2","type":"blocks","created_at":"2025-12-03T12:01:42.495883136-05:00","created_by":"andy"},{"issue_id":"yam-c81.3","depends_on_id":"yam-c81.1","type":"blocks","created_at":"2025-12-03T12:01:42.496667739-05:00","created_by":"andy"}]}
{"id":"yam-c81.3.1","title":"Integrate whisper-rs for file transcription","description":"## Context\nFirst STT test - transcribe a .wav file to text.\nProves whisper-rs builds and runs before adding live audio.\n\n## Implementation Details\nIn `yammer-stt`:\n1. Load Whisper model from downloaded GGML file\n2. Load audio from WAV file (16kHz mono f32)\n3. Run transcription\n4. Return text segments with timestamps\n\n```rust\nlet ctx = WhisperContext::new_with_params(\n    model_path,\n    WhisperContextParameters::default()\n)?;\n\nlet params = FullParams::new(SamplingStrategy::Greedy { best_of: 1 });\nlet mut state = ctx.create_state()?;\nstate.full(params, \u0026audio_data)?;\n\nfor i in 0..state.full_n_segments()? {\n    let text = state.full_get_segment_text(i)?;\n    println!(\"{}\", text);\n}\n```\n\n## Acceptance Criteria\n- [ ] Load whisper-base.en model\n- [ ] Transcribe WAV file\n- [ ] CLI command: `yammer-cli transcribe test.wav`\n- [ ] Output includes timestamps and text\n- [ ] CUDA build works (optional flag)\n\n## Testing\nRecord yourself saying something known, transcribe, verify accuracy.\n\n## Notes\nCUDA build: `CUDA_ARCHITECTURES=86 cargo build --release --features cuda`\nCPU fallback should also work for testing.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-03T12:02:03.633337805-05:00","updated_at":"2025-12-03T12:43:47.597844419-05:00","closed_at":"2025-12-03T12:43:47.597844419-05:00","close_reason":"Implemented Whisper transcription with CLI command. Model loads, transcribes WAV files, outputs timestamps.","dependencies":[{"issue_id":"yam-c81.3.1","depends_on_id":"yam-c81.3","type":"parent-child","created_at":"2025-12-03T12:02:03.63413764-05:00","created_by":"andy"},{"issue_id":"yam-c81.3.1","depends_on_id":"yam-c81.1.2","type":"blocks","created_at":"2025-12-03T12:09:57.614023141-05:00","created_by":"andy"}]}
{"id":"yam-c81.3.2","title":"Implement streaming transcription from live audio","description":"## Context\nReal dictation requires processing live audio, not just files.\nwhisper.cpp processes 30-second segments - we chunk accordingly.\n\n## Implementation Details\nStreaming architecture:\n1. Audio capture fills ring buffer continuously\n2. VAD detects speech segments\n3. When speech ends (or buffer hits ~5s), send chunk to Whisper\n4. Display partial results while capturing continues\n\nChunk timing considerations:\n- Too short (\u003c2s): Poor accuracy, too many API calls\n- Too long (\u003e10s): High latency before seeing results\n- Sweet spot: 3-5 seconds of speech per chunk\n\nThread architecture:\n- Audio thread: cpal callback fills buffer\n- VAD thread: monitors buffer, detects utterances\n- STT thread: processes complete utterances\n\n## Acceptance Criteria\n- [ ] Transcribe live microphone input\n- [ ] VAD triggers transcription on speech end\n- [ ] Results appear within ~1s of speaking\n- [ ] CLI command: `yammer-cli dictate` (speak, see text appear)\n- [ ] No buffer underruns or audio glitches\n\n## Testing\nRun `yammer-cli dictate`, speak naturally, verify text appears promptly.\nTry different speaking speeds, pauses between sentences.\n\n## Notes\nThis is the core dictation loop - everything else builds on this.","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-03T12:02:35.862024804-05:00","updated_at":"2025-12-03T12:47:05.758724371-05:00","closed_at":"2025-12-03T12:47:05.758724371-05:00","close_reason":"Implemented live dictation with VAD-triggered transcription. CLI command yammer dictate works.","dependencies":[{"issue_id":"yam-c81.3.2","depends_on_id":"yam-c81.3","type":"parent-child","created_at":"2025-12-03T12:02:35.863575976-05:00","created_by":"andy"},{"issue_id":"yam-c81.3.2","depends_on_id":"yam-c81.3.1","type":"blocks","created_at":"2025-12-03T12:02:35.864649965-05:00","created_by":"andy"}]}
{"id":"yam-c81.4","title":"Phase 4: LLM Text Correction","description":"Prove out llama_cpp-rs for dictation text correction.\n\n## Goals\n- Load small LLM model\n- Design prompt for dictation correction\n- Test correction quality vs latency tradeoff\n- Validate VRAM usage when both models loaded\n\n## Why Phase 4\nCan be developed in parallel with Phase 3 (only needs Phase 1 models).\nLLM corrects common dictation errors:\n- Homophones (their/there/they're)\n- Punctuation insertion\n- Capitalization\n- Filler word removal","status":"open","priority":0,"issue_type":"epic","created_at":"2025-12-03T12:02:51.064602525-05:00","updated_at":"2025-12-03T12:02:51.064602525-05:00","dependencies":[{"issue_id":"yam-c81.4","depends_on_id":"yam-c81","type":"parent-child","created_at":"2025-12-03T12:02:51.06548955-05:00","created_by":"andy"},{"issue_id":"yam-c81.4","depends_on_id":"yam-c81.1","type":"blocks","created_at":"2025-12-03T12:02:51.066894824-05:00","created_by":"andy"}]}
{"id":"yam-c81.4.1","title":"Integrate llama_cpp-rs for text correction","description":"## Context\nSmall LLM fixes dictation errors that Whisper introduces.\nGoal: fast correction (~200ms) for real-time feel.\n\n## Implementation Details\nIn `yammer-llm`:\n1. Load small GGUF model (Phi-3-mini or Qwen2-1.5B)\n2. Design correction prompt\n3. Generate corrected text\n4. Parse response\n\nPrompt template:\n```\nFix any transcription errors in the following dictation.\nOnly fix obvious mistakes, don't rephrase.\nAdd punctuation and capitalization.\n\nInput: [raw whisper output]\nOutput:\n```\n\nModel selection considerations:\n- Phi-3-mini (3.8B): Higher quality, ~200ms/sentence on GPU\n- Qwen2-1.5B: Faster, ~100ms/sentence, slightly lower quality\n- Both fit in 8GB VRAM alongside Whisper small\n\n## Acceptance Criteria\n- [ ] Load GGUF model with llama_cpp-rs\n- [ ] CLI command: `yammer-cli correct \"their going too the store\"`\n- [ ] Output: \"They're going to the store.\"\n- [ ] Measure latency per correction\n- [ ] CUDA acceleration works\n\n## Testing\nFeed known-bad transcriptions, verify corrections are accurate.\nMeasure latency on typical sentence lengths (10-30 words).\n\n## Notes\nCorrection is optional - user can disable if latency is too high.\nConsider caching corrections for repeated phrases.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-03T12:03:11.262209153-05:00","updated_at":"2025-12-03T13:55:45.40108358-05:00","dependencies":[{"issue_id":"yam-c81.4.1","depends_on_id":"yam-c81.4","type":"parent-child","created_at":"2025-12-03T12:03:11.263414932-05:00","created_by":"andy"},{"issue_id":"yam-c81.4.1","depends_on_id":"yam-c81.1.2","type":"blocks","created_at":"2025-12-03T12:10:02.69166664-05:00","created_by":"andy"},{"issue_id":"yam-c81.4.1","depends_on_id":"yam-ijc","type":"blocks","created_at":"2025-12-03T13:55:05.78505376-05:00","created_by":"andy"}]}
{"id":"yam-c81.4.2","title":"Measure and optimize VRAM usage","description":"## Context\nBoth Whisper and LLM need GPU memory. Must verify they fit together.\n\n## Implementation Details\nTest configurations for 8GB VRAM:\n1. Both models loaded simultaneously\n2. Sequential loading (drop one before loading other)\n3. Different model sizes/quantizations\n\nExpected VRAM usage:\n- Whisper base.en: ~142 MB\n- Whisper small.en: ~466 MB\n- LLM 3B Q4_K_M: ~2 GB\n- KV cache (2048 ctx): ~0.5-1 GB\n- CUDA overhead: ~400 MB\n\n## Acceptance Criteria\n- [ ] Measure actual VRAM usage with nvidia-smi\n- [ ] Document working model combinations for 8GB GPU\n- [ ] Implement sequential loading fallback if needed\n- [ ] CLI command: `yammer-cli gpu-info` shows available/used VRAM\n\n## Testing\nLoad both models, run inference, monitor nvidia-smi.\nTest with different model size combinations.\n\n## Notes\nIf both don't fit, implement hot-swapping:\ntranscribe → drop whisper → load LLM → correct → drop LLM → load whisper","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-03T12:03:29.24262512-05:00","updated_at":"2025-12-03T12:03:29.24262512-05:00","dependencies":[{"issue_id":"yam-c81.4.2","depends_on_id":"yam-c81.4","type":"parent-child","created_at":"2025-12-03T12:03:29.243460627-05:00","created_by":"andy"},{"issue_id":"yam-c81.4.2","depends_on_id":"yam-c81.4.1","type":"blocks","created_at":"2025-12-03T12:03:29.244518137-05:00","created_by":"andy"},{"issue_id":"yam-c81.4.2","depends_on_id":"yam-c81.3.1","type":"blocks","created_at":"2025-12-03T12:03:29.245311512-05:00","created_by":"andy"}]}
{"id":"yam-c81.5","title":"Phase 5: Tauri UI","description":"Build the floating overlay UI with Tauri.\n\n## Goals\n- Chromeless transparent window with rounded corners\n- Real-time waveform visualization\n- Status indicators (listening, processing, idle)\n- Transcription display\n\n## Why Phase 5\nDepends on working audio pipeline (for waveform) and STT (for transcription).\nUI is presentation layer over proven building blocks.","status":"open","priority":0,"issue_type":"epic","created_at":"2025-12-03T12:03:49.436718685-05:00","updated_at":"2025-12-03T12:03:49.436718685-05:00","dependencies":[{"issue_id":"yam-c81.5","depends_on_id":"yam-c81","type":"parent-child","created_at":"2025-12-03T12:03:49.437925582-05:00","created_by":"andy"},{"issue_id":"yam-c81.5","depends_on_id":"yam-c81.3","type":"blocks","created_at":"2025-12-03T12:03:49.439448208-05:00","created_by":"andy"},{"issue_id":"yam-c81.5","depends_on_id":"yam-c81.2","type":"blocks","created_at":"2025-12-03T12:03:49.440437362-05:00","created_by":"andy"}]}
{"id":"yam-c81.5.1","title":"Create basic Tauri window with transparency","description":"## Context\nFirst UI task - get a chromeless transparent window working on X11.\n\n## Implementation Details\nIn `yammer-app`:\n1. Initialize Tauri with window config\n2. Configure for overlay appearance\n3. Add CSS-based rounded corners\n\ntauri.conf.json:\n```json\n{\n  \"windows\": [{\n    \"title\": \"Yammer\",\n    \"width\": 300,\n    \"height\": 100,\n    \"decorations\": false,\n    \"transparent\": true,\n    \"resizable\": false,\n    \"alwaysOnTop\": true,\n    \"skipTaskbar\": true\n  }]\n}\n```\n\nCSS:\n```css\nhtml, body { background: transparent; }\n.app-container {\n  background: rgba(30, 30, 30, 0.95);\n  border-radius: 16px;\n  box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);\n}\n```\n\n## Acceptance Criteria\n- [ ] Window appears floating over other apps\n- [ ] Transparent background with rounded corners\n- [ ] No window decorations (title bar, borders)\n- [ ] Draggable via data-tauri-drag-region\n- [ ] Always-on-top works\n\n## Testing\nRun on X11 (GNOME on Xorg), verify visual appearance.\nDrag window, verify it moves smoothly.\n\n## Notes\nRequires X11 - won't work properly on Wayland.\nCheck with: echo $XDG_SESSION_TYPE","status":"closed","priority":0,"issue_type":"task","created_at":"2025-12-03T12:04:14.646468326-05:00","updated_at":"2025-12-04T18:18:39.208490948-05:00","closed_at":"2025-12-04T18:18:39.208490948-05:00","close_reason":"Closed","dependencies":[{"issue_id":"yam-c81.5.1","depends_on_id":"yam-c81.5","type":"parent-child","created_at":"2025-12-03T12:04:14.647408333-05:00","created_by":"andy"},{"issue_id":"yam-c81.5.1","depends_on_id":"yam-c81.2.3","type":"blocks","created_at":"2025-12-03T12:10:07.768353817-05:00","created_by":"andy"}]}
{"id":"yam-c81.5.2","title":"Implement real-time waveform visualization","description":"## Context\nVisual feedback while speaking - waveform shows audio is being captured.\n\n## Implementation Details\nTwo approaches:\n\nOption A: Web Audio API (capture in browser)\n- Use navigator.mediaDevices.getUserMedia\n- AnalyserNode for frequency/time data\n- Canvas for rendering\n\nOption B: Rust audio → Tauri events\n- Audio captured in Rust (cpal)\n- Emit samples to frontend via app.emit_all()\n- Canvas renders received samples\n\nOption B is preferred (audio already captured in Rust for Whisper).\n\nFrontend (Canvas):\n```javascript\nlisten('audio-samples', (event) =\u003e {\n  const samples = event.payload;\n  drawWaveform(samples);\n});\n\nfunction drawWaveform(samples) {\n  // Draw bars or line graph\n}\n```\n\n## Acceptance Criteria\n- [ ] Waveform updates in real-time while speaking\n- [ ] Smooth 30-60fps rendering\n- [ ] Visual feedback when VAD detects speech\n- [ ] Scales to window size\n\n## Testing\nSpeak into mic, verify waveform responds.\nWave should be active during speech, flat during silence.\n\n## Notes\nConsider WebGL for better performance if Canvas is too slow.\nCould use wavesurfer.js library for styled visualizations.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-03T12:04:46.777186788-05:00","updated_at":"2025-12-03T12:04:46.777186788-05:00","dependencies":[{"issue_id":"yam-c81.5.2","depends_on_id":"yam-c81.5","type":"parent-child","created_at":"2025-12-03T12:04:46.77828417-05:00","created_by":"andy"},{"issue_id":"yam-c81.5.2","depends_on_id":"yam-c81.5.1","type":"blocks","created_at":"2025-12-03T12:04:46.779499393-05:00","created_by":"andy"}]}
{"id":"yam-c81.5.3","title":"Add status indicators and transcription display","description":"## Context\nUser needs to know app state and see transcription results.\n\n## Implementation Details\nUI States:\n1. **Idle** - waiting for hotkey, dimmed appearance\n2. **Listening** - capturing audio, waveform active, green indicator\n3. **Processing** - sent to Whisper, spinner/pulse animation\n4. **Correcting** - LLM processing, different color indicator\n5. **Done** - text ready, briefly show then copy/type\n\nState machine in Rust backend, emit state changes to frontend.\n\nTranscription display:\n- Show partial results as they come in\n- Final result highlighted differently\n- Option to show corrections (strikethrough original)\n\n## Acceptance Criteria\n- [ ] Visual indicator for each state\n- [ ] Smooth state transitions (fade/animation)\n- [ ] Transcription text appears during/after processing\n- [ ] Clear visual when output is ready\n\n## Testing\nGo through all states, verify visual feedback is clear.\nTest rapid state changes (quick utterances).\n\n## Notes\nKeep UI minimal - it's an overlay, not main focus.\nUser's attention should be on their work, not the dictation UI.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-03T12:05:07.118892134-05:00","updated_at":"2025-12-03T12:05:07.118892134-05:00","dependencies":[{"issue_id":"yam-c81.5.3","depends_on_id":"yam-c81.5","type":"parent-child","created_at":"2025-12-03T12:05:07.120128328-05:00","created_by":"andy"},{"issue_id":"yam-c81.5.3","depends_on_id":"yam-c81.5.2","type":"blocks","created_at":"2025-12-03T12:05:07.121174387-05:00","created_by":"andy"}]}
{"id":"yam-c81.6","title":"Phase 6: Integration \u0026 Polish","description":"Wire everything together into a complete dictation experience.\n\n## Goals\n- Global hotkey to start/stop dictation\n- Text output via xdotool (type into focused app)\n- Settings/configuration UI\n- Error handling and edge cases\n\n## Why Phase 6\nFinal integration after all building blocks proven.\nPolish and user experience improvements.","status":"open","priority":0,"issue_type":"epic","created_at":"2025-12-03T12:05:37.678144676-05:00","updated_at":"2025-12-03T12:05:37.678144676-05:00","dependencies":[{"issue_id":"yam-c81.6","depends_on_id":"yam-c81","type":"parent-child","created_at":"2025-12-03T12:05:37.679094045-05:00","created_by":"andy"},{"issue_id":"yam-c81.6","depends_on_id":"yam-c81.5","type":"blocks","created_at":"2025-12-03T12:05:37.680552818-05:00","created_by":"andy"},{"issue_id":"yam-c81.6","depends_on_id":"yam-c81.4","type":"blocks","created_at":"2025-12-03T12:05:37.681571917-05:00","created_by":"andy"}]}
{"id":"yam-c81.6.1","title":"Implement global hotkey for dictation toggle","description":"## Context\nUser presses hotkey anywhere → dictation starts/stops.\nX11 allows global key grabbing natively.\n\n## Implementation Details\nUse global-hotkey crate (from Tauri maintainers):\n\n```rust\nuse global_hotkey::{GlobalHotKeyManager, hotkey::{HotKey, Modifiers, Code}};\n\nlet manager = GlobalHotKeyManager::new()?;\nlet hotkey = HotKey::new(Some(Modifiers::SUPER), Code::KeyD);\nmanager.register(hotkey)?;\n\n// Listen via GlobalHotKeyEvent::receiver()\n```\n\nDefault hotkey: Super+D (configurable)\nBehavior:\n- Press once: Start listening\n- Press again: Stop and process\n- Or: Hold to record, release to process\n\n## Acceptance Criteria\n- [ ] Global hotkey works from any application\n- [ ] Configurable key combination\n- [ ] Works on X11 (required)\n- [ ] Graceful error if key already grabbed by another app\n\n## Testing\nFocus different apps (browser, terminal, editor), press hotkey.\nDictation should activate regardless of focused window.\n\n## Notes\nOnly works on X11. Wayland restricts global key grabs.\nDocument requirement for X11 in README.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-03T12:05:57.672470149-05:00","updated_at":"2025-12-03T12:05:57.672470149-05:00","dependencies":[{"issue_id":"yam-c81.6.1","depends_on_id":"yam-c81.6","type":"parent-child","created_at":"2025-12-03T12:05:57.673402329-05:00","created_by":"andy"},{"issue_id":"yam-c81.6.1","depends_on_id":"yam-c81.5.3","type":"blocks","created_at":"2025-12-03T12:10:12.845606747-05:00","created_by":"andy"}]}
{"id":"yam-c81.6.2","title":"Implement text output via xdotool","description":"## Context\nAfter dictation, type text into the user's focused application.\nxdotool simulates keyboard input on X11.\n\n## Implementation Details\nPrimary method - xdotool type:\n```rust\nfn type_text(text: \u0026str) -\u003e io::Result\u003c()\u003e {\n    Command::new(\"xdotool\")\n        .args([\"type\", \"--clearmodifiers\", \"--\", text])\n        .status()?;\n    Ok(())\n}\n```\n\nFallback - clipboard + paste:\n```rust\nfn paste_text(text: \u0026str) -\u003e io::Result\u003c()\u003e {\n    let mut clipboard = Clipboard::new()?;\n    clipboard.set_text(text)?;\n    Command::new(\"xdotool\")\n        .args([\"key\", \"ctrl+v\"])\n        .status()?;\n    Ok(())\n}\n```\n\nThe `--clearmodifiers` flag is important - it releases any held keys\n(like Super from the hotkey) before typing.\n\n## Acceptance Criteria\n- [ ] Type text into any focused application\n- [ ] Handle special characters correctly\n- [ ] Clipboard fallback for problematic apps\n- [ ] Works after hotkey release (modifiers cleared)\n\n## Testing\nDictate into: terminal, browser text field, code editor.\nVerify text appears correctly in all.\n\n## Notes\nRequires: `sudo apt install xdotool`\nSome apps (electron, some terminals) work better with clipboard paste.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-03T12:06:33.900711398-05:00","updated_at":"2025-12-03T12:06:33.900711398-05:00","dependencies":[{"issue_id":"yam-c81.6.2","depends_on_id":"yam-c81.6","type":"parent-child","created_at":"2025-12-03T12:06:33.901778395-05:00","created_by":"andy"},{"issue_id":"yam-c81.6.2","depends_on_id":"yam-c81.5.3","type":"blocks","created_at":"2025-12-03T12:10:17.926180677-05:00","created_by":"andy"}]}
{"id":"yam-c81.6.3","title":"Wire full dictation pipeline","description":"## Context\nConnect all building blocks into the complete dictation flow.\n\n## Implementation Details\nFull pipeline:\n1. User presses hotkey → UI shows \"Listening\"\n2. Audio capture starts, waveform animates\n3. VAD detects speech segments\n4. On speech end (or hotkey release):\n   - UI shows \"Processing\"\n   - Send audio to Whisper\n5. Whisper returns text:\n   - UI shows \"Correcting\" (if LLM enabled)\n   - Send to LLM for correction\n6. Final text ready:\n   - UI shows result briefly\n   - Type into focused app via xdotool\n   - UI returns to \"Idle\"\n\nError handling:\n- Whisper timeout → show error, don't output partial\n- LLM timeout → output uncorrected text\n- xdotool fails → copy to clipboard, notify user\n\n## Acceptance Criteria\n- [ ] Complete flow works end-to-end\n- [ ] Latency from speech-end to typed text \u003c 2s (GPU)\n- [ ] Graceful degradation on errors\n- [ ] Interruptible (hotkey cancels mid-process)\n\n## Testing\nDictate sentences of varying length, verify accuracy and latency.\nTest interruption mid-processing.\nTest with LLM correction on/off.\n\n## Notes\nThis is the integration task - all building blocks already work.\nFocus on glue code and error handling.","status":"open","priority":0,"issue_type":"task","created_at":"2025-12-03T12:06:51.753570348-05:00","updated_at":"2025-12-03T12:06:51.753570348-05:00","dependencies":[{"issue_id":"yam-c81.6.3","depends_on_id":"yam-c81.6","type":"parent-child","created_at":"2025-12-03T12:06:51.754473037-05:00","created_by":"andy"},{"issue_id":"yam-c81.6.3","depends_on_id":"yam-c81.6.1","type":"blocks","created_at":"2025-12-03T12:06:51.755663633-05:00","created_by":"andy"},{"issue_id":"yam-c81.6.3","depends_on_id":"yam-c81.6.2","type":"blocks","created_at":"2025-12-03T12:06:51.756527669-05:00","created_by":"andy"}]}
{"id":"yam-c81.6.4","title":"Add configuration and settings","description":"## Context\nUsers need to configure hotkey, model selection, correction toggle, etc.\n\n## Implementation Details\nConfig file: ~/.config/yammer/config.toml\n\n```toml\n[hotkey]\nmodifiers = [\"Super\"]\nkey = \"D\"\n\n[models]\nwhisper = \"small.en\"  # tiny, base, small, medium\nllm = \"phi-3-mini\"    # or \"none\" to disable correction\n\n[audio]\nvad_threshold = 0.02\nvad_speech_frames = 3\nvad_silence_frames = 15\n\n[output]\nmethod = \"type\"  # or \"clipboard\"\n```\n\nSettings UI (optional, lower priority):\n- In-app settings panel\n- Or: edit config file, restart app\n\n## Acceptance Criteria\n- [ ] Load config from TOML file\n- [ ] Sensible defaults if no config exists\n- [ ] CLI to print current config: `yammer --config`\n- [ ] Document all config options\n\n## Testing\nModify config, restart app, verify changes apply.\n\n## Notes\nKeep config simple initially. Add settings UI later if needed.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-03T12:07:18.075150406-05:00","updated_at":"2025-12-03T12:07:18.075150406-05:00","dependencies":[{"issue_id":"yam-c81.6.4","depends_on_id":"yam-c81.6","type":"parent-child","created_at":"2025-12-03T12:07:18.075897965-05:00","created_by":"andy"},{"issue_id":"yam-c81.6.4","depends_on_id":"yam-c81.6.3","type":"blocks","created_at":"2025-12-03T12:07:18.077187187-05:00","created_by":"andy"}]}
{"id":"yam-ijc","title":"Install clang and libclang-dev for llama_cpp build","description":"llama_cpp_sys requires clang/libclang for bindgen to generate Rust bindings from C headers. Missing stdbool.h suggests libclang-dev is not installed.\n\nRequired packages (Ubuntu):\n```\nsudo apt install clang libclang-dev\n```\n\nError encountered:\n```\nfatal error: 'stdbool.h' file not found\nUnable to generate bindings: ClangDiagnostic\n```","status":"open","priority":0,"issue_type":"chore","created_at":"2025-12-03T13:54:56.780423151-05:00","updated_at":"2025-12-03T13:54:56.780423151-05:00"}
