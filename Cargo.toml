[workspace]
resolver = "2"
members = [
    "yammer-core",
    "yammer-audio",
    "yammer-stt",
    "yammer-llm",
    "yammer-output",
    "yammer-cli",
    "yammer-app/src-tauri",
]

[workspace.package]
version = "0.2.0"
edition = "2024"
authors = ["Andy"]
license = "MIT"
repository = "https://github.com/andy/yammer"

[workspace.dependencies]
# Common dependencies shared across crates
thiserror = "2"
anyhow = "1"
tokio = { version = "1", features = ["full"] }
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
serde = { version = "1", features = ["derive"] }
serde_json = "1"
reqwest = { version = "0.12", features = ["stream"] }
sha2 = "0.10"
futures-util = "0.3"
indicatif = "0.17"
clap = { version = "4", features = ["derive"] }
cpal = "0.15"
hound = "3"
ringbuf = "0.4"
rubato = "0.16"
ctrlc = "3"

# GPU Acceleration Configuration
# -----------------------------
# whisper-rs and llama_cpp both bundle ggml with CUDA support. When both enable CUDA,
# the linker fails with duplicate symbol errors (ggml_cuda_error, etc.) because each
# library compiles its own copy of ggml-cuda.cu.
#
# Current workaround: Enable CUDA only for whisper-rs (STT) since real-time audio
# transcription benefits most from GPU acceleration. LLM runs on CPU which is
# acceptable for text cleanup tasks.
#
# Future options:
# - Vulkan backend: Both libraries support vulkan feature (may avoid conflicts)
# - Separate processes: Run STT and LLM in different processes
# - Upstream fix: Wait for ggml to be a proper shared dependency
whisper-rs = { version = "0.14", features = ["cuda"] }
llama_cpp = { version = "0.3" }  # CPU-only to avoid duplicate CUDA symbols
